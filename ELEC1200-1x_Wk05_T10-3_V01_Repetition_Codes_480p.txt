So let's talk about repetition codes, which are one type of block code.
So the repetition code is simply the idea of repeating what I said.
So if I'm going to transmit a 1, what I'll do with the repetition code is
simply repeat it, let's say, n times.
So a (2, 1, 2) repetition code would be taking the message bit, 0 or 1.
My message block is just one-bit long,
and I'm going to repeat it twice.
So the first codeword would be 0 0 that I would send,
and the second codeword corresponding 1 would be 1 1.
And then, in that case, since I'm sending
two bits for every one-bit of message, the code rate is 1/2.
And for the (3, 1, 3) repetition code, what I'm going to do
is I'm just going to repeat it three times.
And so, then, the code rate similarly goes down to 1/3.
Now, let's take a look at those two codes, the (2, 1, 2) and the (3, 1, 3)
code, and look at what the error detection and error correction
capabilities of those are.
So, with the (2, 1, 2) repetition code, what we do is we repeat each bit twice.
So the message bit 0 gets transmitted as 0 0, and the message bit 1
gets transmitted as 1 1.
And so what we notice inside here is that there's always
an even number of 1s. Either there's zero 1s or there are two 1s. And so we
give this kind of code word a special name.
We say that this is even parity, because there's always
an even number of 1s inside the codewords.
Now, in fact, because of bit errors, even though we send these two bits,
we might observe something else, because bit errors might occur.
In fact, there are four total possible combinations that we might observe,
because there are two bits and each of them have two possibilities.
Each of these lines, then, show you a single bit transition between these.
So if I go from here to here, I'm just changing a single bit.
And from here to here.
So we can see, in this case, for the (2, 1, 2) repetition code,
that there is a Hamming distance of two between 1 1 and 0 0.
We can see that number one because they differ in two places.
And the other way we can see that is because, in order to get from 1 1
to 0 0, I have to go through two of these transitions, like this.
So, then, it turns out that we can actually do detection in this one,
because if we see anything other than 1 1 or 0 0,
we know that an error has occurred.
So, in fact, because the Hamming distance is two,
we know that we can detect up to d minus one or two minus one or one-bit of error.
So if we transmitted this and there's a one-bit error,
then either the bit error occurred in the first position, in which case
I would receive this, or the bit error occurred
in the second position, in which case I receive this.
But whenever there's a single bit error, I'll receive something that has
a mixture of 0s and 1s.
That will also change the number of 1s so that they are odd.
Now, the problem here, though, is that I cannot correct,
because suppose I received 1 0.
It's equidistant from 1 1 and 0 0, because I
don't know whether the error occurred in the second bit, in which case
I would have sent 1 1, or in the first bit, in which case I would have sent 0 0.
So in order to then correct errors, what I'm going to have to do
is increase the distance between codewords.
And that's exactly what the (3, 1, 3) repetition code does.
So with the (3, 1, 3) repetition code, again, what I'm going to be doing
is sending one-bit messages, so either 0 or 1, but I'm going to repeat that
0 or 1 three times.
So I'm going to send either 0 0 0 or 1 1 1.
Now, the Hamming distance between the two codewords I can see by inspection
is d is equal to three, because this one differs
from this one in three positions.
Another way I can see it is by looking at all possible things
that I might receive.
So because there are three bit positions and each of them
can assume one of two possible values, I have eight total possibilities,
which I show over here in this cube-like structure, where
each of the edges of the cube indicates a single bit change.
So, for example, if I move from here to here along this edge,
I've just changed the middle bit.
If I move from here to here, I've just changed the end bit and so on.
And so another way we can see that the Hamming distance between this codeword
and this codeword is three is by the idea
that no matter how I want to get from here to here by traveling along
edges of this cube, I always have go through one, two, three; one, two, three; one, two, three.
So no matter how I go, the distance is always at least three.
In this case, it's always exactly three.
And so the Hamming distance, then, tells us
about the error correction or error detection capabilities of this code.
So we can, remember, either choose to detect errors or correct errors.
We can detect errors up to d minus one or three minus one, or two bits.
So that means we can detect one or two bit errors.
Or we can detect and correct errors up to about half
of that, so d minus one or two divided by two, or one bit.
And so let's talk about detection first.
So if we want to detect errors, then what we're really going to look for
is whenever we get something that is not this or this,
then we know that an error has occurred.
And, in particular, if we look at these guys
here, which correspond to possible errors,
we see that there's a mixture of 0s and 1s inside here.
So if we look at this guy, here, 1 0 0, then we can say,
well, there's two possibilities.
Either you sent 0 0 0 and there was a one-bit change
to get to here, there was a one-bit change in the first one,
or you sent 1 1 1, and there was a two-bit change,
so that these two bits over here were sent.
And so if I assume that either one or two-bit errors can occur,
then there's no way I can decide whether to go to this guy
or to this guy, because both of them are possibilities.
So, then, in order to actually correct, what I have to do
is I have to make an assumption that only one-bit errors occur.
So if I assume that only one-bit errors occur, then if I receive 1 0 0,
then I know that it must have come from 0 0 0,
because it couldn't have come from here, because in order for me
to get from here to here, there were two-bit changes I have to make.
And so when I'm going to do correction, in this case,
I get this very, very nice, intuitive idea that what I'm going to do
is simply do some kind of voting.
I'm going to look at the received codeword,
and I'm going to see how many votes I have for each bit.
So, in this case here, for all of these three guys, which are one-bit
away from 0 0 0, I look, and I see two out of three of the bits
received are 0.
And for these guys over here, two out of three are 1.
So I'm just going to look at the received codeword,
see which bit has the most votes, and then just decide
that was what the original message bit was.
Now, I can further increase the Hamming distance
by going to a 4 1 4 repetition code.
And so now, given this, remember, the Hamming distance, now,
would increase by one, and the detection would then also increase to d minus one,
or three bits.
And if I use that formula and I say d minus one, or three divided by two,
I get 1.5 bits.
So that says that I can detect and correct errors in up to 1.5 bits.
But what does that mean?
What does it mean that I can detect and correct up to 1.5?
What is a half bit?
And so, obviously, this is going to be a little bit better than detecting
and correcting one-bit errors, but not quite as good as detecting and correcting
two-bit errors.
And so, in fact, what we have is this halfway state where
we can detect and correct one-bit errors, and we can detect but not
correct two-bit errors.
And to see that, let's take a look at this example.
Now in this case, here, because I have four bits,
and I can't draw this kind of diagram of all the possibilities,
because I'm now living in a four-dimensional space.
And it's very, very hard to draw in a four dimensional space.
So I'll just show you one possible set of codewords from this space.
So here are the two messages bits corresponding to 0 and 1.
All I've done is repeated them four times.
And here are some example codewords that I might receive.
And these are all separated by one-bit transitions.
So, here, I have a transition in the first position,
a transition in the last position, a transition in the second position,
and then a final transition in the third position to get over there.
Suppose that I observe 1 0 0 0.
Then, in that case, there are two possibilities.
If I assume that I can detect errors in up to three bits.
Either I had a one-bit error from 0 0 0 0, or I had a three-bit error from 1 1 1 1.
And if I say, well, this one is closer to this one,
and I choose to correct, then, essentially, what I'm doing
is I'm saying, well, three-bit errors don't ever occur,
because if a three-bit error did occur, then I
would be correcting this in the wrong direction.
Now, suppose, though, I have observed 1 0 0 1.
In this case, here, there's a two-bit error from here,
and there's also a two-bit error from here.
And now, because that 1 0 0 1 is equidistant from 0 0 0 0 and 1 1 1 1,
I cannot decide which one to go to.
And so, therefore, I can only detect the two-bit error, but I cannot correct it.
So this is very similar to the case when we talked about "nive."
That's halfway between five and nine.
And so I can detect that you didn't say a number,
but I don't know which one you meant to say.